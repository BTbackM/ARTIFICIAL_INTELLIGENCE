{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import own libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader.mnist import MNISTDataset\n",
    "from model.mnist_cnn import MnistCNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = datasets.MNIST(root = '../data', train = True, transform = transforms.ToTensor(), download = True)\n",
    "test_set = datasets.MNIST(root = '../data', train = False, transform = transforms.ToTensor(), download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of train set: 60000\n",
      "Lenght of test set: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f'Lenght of train set: {len(train_set)}')\n",
    "print(f'Lenght of test set: {len(test_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([60000, 28, 28])\n",
      "Target shape: torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "# Train set shapes\n",
    "print(f'Data shape: {train_set.data.shape}')\n",
    "print(f'Target shape: {train_set.targets.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size = batch_size, shuffle = True)\n",
    "test_dataloader = DataLoader(test_set, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MnistCNN()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 20], Loss: 0.0233\n",
      "Epoch [2 / 20], Loss: 0.0250\n",
      "Epoch [3 / 20], Loss: 0.1844\n",
      "Epoch [4 / 20], Loss: 0.0259\n",
      "Epoch [5 / 20], Loss: 0.1155\n",
      "Epoch [6 / 20], Loss: 0.2617\n",
      "Epoch [7 / 20], Loss: 0.0016\n",
      "Epoch [8 / 20], Loss: 0.0318\n",
      "Epoch [9 / 20], Loss: 0.0813\n",
      "Epoch [10 / 20], Loss: 0.0376\n",
      "Epoch [11 / 20], Loss: 0.0085\n",
      "Epoch [12 / 20], Loss: 0.0021\n",
      "Epoch [13 / 20], Loss: 0.0098\n",
      "Epoch [14 / 20], Loss: 0.0001\n",
      "Epoch [15 / 20], Loss: 0.0000\n",
      "Epoch [16 / 20], Loss: 0.0001\n",
      "Epoch [17 / 20], Loss: 0.0016\n",
      "Epoch [18 / 20], Loss: 0.0017\n",
      "Epoch [19 / 20], Loss: 0.0000\n",
      "Epoch [20 / 20], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    for features, labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(features)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print the training loss after each epoch\n",
    "    print(f'Epoch [{epoch + 1} / {num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.91%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for features, labels in test_dataloader:\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f\"Test Accuracy: {100 * correct / total}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
